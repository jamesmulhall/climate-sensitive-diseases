{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_dengue_fever",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja18nXEglgwF"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW4v2IEcTYHa"
      },
      "source": [
        "# Install packages\n",
        "!pip install -U scikit-learn\n",
        "!pip install ftfy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8I8S9koHjT_"
      },
      "source": [
        "# Imports\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from ftfy import fix_text\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [7, 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNAuUk6_HVs1"
      },
      "source": [
        "# Attach Google Drive for reading and saving files\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/My Drive/PROJECT/HealthCare/FINAL_RESULTS/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRLJetKppxRj"
      },
      "source": [
        "%%capture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ETIH62MHpiC"
      },
      "source": [
        "# Read in data\n",
        "full_data = pd.read_excel(\"./full_data_fixed.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVkHGeUPO6cO"
      },
      "source": [
        "#@title Seed\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    \n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEn56jmQtEmD"
      },
      "source": [
        "# Set hyperparameters as args using the Configuration class\n",
        "class Configuration():\n",
        "    def __init__(self):\n",
        "        self.test_size = 36\n",
        "        self.look_back = 3\n",
        "        self.n_predicted_month = 3\n",
        "        self.n_features = 3\n",
        "        self.seed = 42\n",
        "        self.batch_size = 16\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.epochs = 300\n",
        "\n",
        "args = Configuration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukoPuS5CHvdl"
      },
      "source": [
        "# Define data (pre-)processing functions\n",
        "def get_city_data(city_name):\n",
        "    \"\"\"Returns DF rate and climate data\"\"\"\n",
        "    return full_data.loc[full_data['province'] == city_name].drop(columns=['Influenza_cases', 'Dengue_fever_cases', 'Diarrhoea_cases', 'Influenza_rates', 'Diarrhoea_rates', 'province', 'year_month'], \n",
        "                                                                  axis=1, \n",
        "                                                                  inplace=False)\n",
        "\n",
        "def convert_to_stationary(city_data):\n",
        "     \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
        "    for col_name in city_data.columns:\n",
        "        if col_name != 'Dengue_fever_rates':\n",
        "            try:\n",
        "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
        "            except:\n",
        "                print(col_name)\n",
        "    return city_data\n",
        "\n",
        "def impute_missing_value(city_data):\n",
        "    \"\"\"\n",
        "    Imputes 0 for first 12 months, \n",
        "    last year's value for months 12-24, \n",
        "    and minimum value of last two years for months 25+\n",
        "    \"\"\"\n",
        "    for col in city_data.columns:\n",
        "        for index in range(len(city_data[col])):\n",
        "            if np.isnan(city_data[col].iloc[index]):\n",
        "                if index < 12:\n",
        "                    city_data[col].iloc[index] = 0\n",
        "                elif index >= 12 and index <= 24:\n",
        "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
        "                else:\n",
        "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
        "    return city_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT8LmtHts4fQ"
      },
      "source": [
        "def split_data(data):\n",
        "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
        "    train = data[: -args.test_size]\n",
        "    test = data[-args.test_size - args.look_back: ]\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wdc44U0uMbP"
      },
      "source": [
        "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
        "  \"\"\"\n",
        "  Frames time-series as supervised learning dataset.\n",
        "  \n",
        "  Args:\n",
        "    d_in: lookback window\n",
        "    d_out: number of predicted months\n",
        "    features_list: list of all features **where last col is the disease incidence**\n",
        "\n",
        "  Returns:\n",
        "    Numpy arrays of disease incidence (y) and other predictors (X)\n",
        "  \"\"\"\n",
        "    X, y = list(), list()\n",
        "    for index, _ in enumerate(data):\n",
        "        in_end = index + d_in\n",
        "        out_end = in_end + d_out\n",
        "        if out_end <= len(data):\n",
        "            if len(features_list) == 0 :\n",
        "                X.append(data[index: in_end, :])\n",
        "            else:\n",
        "                X.append(data[index: in_end, features_list])\n",
        "            y.append(data[in_end: out_end, -1])\n",
        "    return np.array(X), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_RfqYtVGTAk"
      },
      "source": [
        "def select_feature(train, specific_data):\n",
        "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
        "    train_X, train_y = to_supervised(train, d_in=1, d_out=1)\n",
        "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
        "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
        "    fit = rfe.fit(train_X, train_y)\n",
        "    important_features = list()\n",
        "    # print(\"Important Feature:\")\n",
        "    for i in range(len(fit.support_)):\n",
        "        if fit.support_[i]:\n",
        "            important_features.append(i)\n",
        "            # print(specific_data.columns[i])\n",
        "    return np.array(important_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsgC3mbHV96C"
      },
      "source": [
        "def get_data(train_np, test_np, batch_size, specific_data):\n",
        "    \"\"\"\n",
        "    Returns important feature list and data formatted for input into Pytorch \n",
        "    models\n",
        "    \"\"\"\n",
        "    important_features = select_feature(train_np, specific_data)\n",
        "\n",
        "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
        "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
        "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
        "\n",
        "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return important_features, train_loader, test_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW18DMZgF5K_"
      },
      "source": [
        "#Define Pytorch LSTM model\n",
        "class MultiVariateLSTM(nn.Module):\n",
        "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
        "        super(MultiVariateLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
        "    \n",
        "    def forward(self, X_batch, y_batch=None):\n",
        "        output, (last_hidden, _) = self.lstm(X_batch)\n",
        "        last_hidden_vector = output[:, -1, :]\n",
        "        y_predicted = self.linear(last_hidden_vector)\n",
        "        if y_batch != None:\n",
        "            assert y_predicted.size() == y_batch.size()\n",
        "            loss = self.loss_fn(y_predicted, y_batch)\n",
        "            loss = 0.5 * loss / self.sigma**2\n",
        "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
        "            return y_predicted, loss\n",
        "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
        "        else:\n",
        "            return y_predicted\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X, device=args.device)\n",
        "        return self.forward(X)\n",
        "\n",
        "#Define Pytorch LSTM-ATT model\n",
        "class MultiVariateLSTM_Attention(nn.Module):\n",
        "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
        "        super(MultiVariateLSTM_Attention, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
        "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
        "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
        "    \n",
        "    def forward(self, X_batch, y_batch=None):\n",
        "        output, (last_hidden, _) = self.lstm(X_batch)\n",
        "        last_hidden_vector = last_hidden[-1]\n",
        "        remain_hidden_vector = output\n",
        "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
        "        alpha_t = F.softmax(e_t, dim=1)\n",
        "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
        "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
        "        # combine_vector = last_hidden_vector + attenion_vector\n",
        "        y_predicted = self.linear(attenion_vector)\n",
        "        if y_batch != None:\n",
        "            assert y_predicted.size() == y_batch.size()\n",
        "            loss = self.loss_fn(y_predicted, y_batch)\n",
        "            loss = 0.5 * loss / self.sigma**2\n",
        "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
        "            return y_predicted, loss\n",
        "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
        "        else:\n",
        "            return y_predicted\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X, device=args.device)\n",
        "        return self.forward(X)\n",
        "\n",
        "# Define Pytorch CNN model\n",
        "class MultivariateCNN(nn.Module):\n",
        "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
        "        super(MultivariateCNN, self).__init__()\n",
        "        self.loss_fn = loss = nn.MSELoss()\n",
        "        self.filter_sizes = [1, 2, 3]\n",
        "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
        "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
        "\n",
        "    def forward(self, X_batch, y_batch=None):\n",
        "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
        "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
        "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
        "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
        "        y_predicted = self.linear(self.dropout(X_fc))\n",
        "        if y_batch != None:\n",
        "            assert y_predicted.size() == y_batch.size()\n",
        "            loss = self.loss_fn(y_predicted, y_batch)\n",
        "            loss = 0.5 * loss / self.sigma**2\n",
        "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
        "            return y_predicted, loss\n",
        "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
        "        else:\n",
        "            return y_predicted\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X, device=args.device)\n",
        "        return self.forward(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWNY7faYe52P"
      },
      "source": [
        "# Create class to train and evaluate models\n",
        "class Trainer():\n",
        "    def __init__(self, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
        "        \"\"\"\n",
        "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
        "        hyperparameters. Adam optimiser used for all models.\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout)\n",
        "        self.model.double().to(args.device)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
        "    \n",
        "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout):\n",
        "      \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
        "        if model_type.lower() == 'lstm':\n",
        "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
        "        elif model_type.lower() == 'lstm_attention':\n",
        "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
        "        elif model_type.lower() == 'cnn':\n",
        "            model = MultivariateCNN(num_filters, dropout)\n",
        "        return model\n",
        "\n",
        "    def step(self, batch):\n",
        "        self.model.train()\n",
        "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
        "        self.optimizer.zero_grad()\n",
        "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.mean().item()\n",
        "\n",
        "    def validation(self):\n",
        "        self.model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        result = {}\n",
        "\n",
        "        y_true = np.array([])\n",
        "        y_pred = np.array([])\n",
        "\n",
        "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
        "        with torch.no_grad():\n",
        "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
        "            eval_loss = loss.mean().item()\n",
        "\n",
        "        return eval_loss\n",
        "\n",
        "    def train(self, epochs=20):\n",
        "        best_lost = float(\"inf\")\n",
        "        best_model = None\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            for batch in self.train_loader:\n",
        "                loss = self.step(batch)\n",
        "                total_loss += loss\n",
        "            train_loss = total_loss/len(self.train_loader)\n",
        "            eval_loss = self.validation()\n",
        "            if eval_loss < best_lost:\n",
        "                best_lost = eval_loss\n",
        "                best_model = copy.deepcopy(self.model)\n",
        "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
        "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
        "        self.model = best_model\n",
        "        self.model.eval()\n",
        "        return None\n",
        "    \n",
        "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None):\n",
        "        assert scaled, \"data must be scaled\"\n",
        "        self.model.eval()\n",
        "        tensor_data = torch.from_numpy(np_data)\n",
        "        rmse_list = []\n",
        "        mae_list = [] \n",
        "        mape_list = []\n",
        "\n",
        "        y_predicted_list = []\n",
        "        y_true_list = []\n",
        "\n",
        "        for k_steps in range(1, args.n_predicted_month + 1):\n",
        "            y_predicted = []\n",
        "            y_true = []\n",
        "            for index in range(tensor_data.size(0) - args.look_back):\n",
        "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
        "                # yhat = self.model.predict(X.unsqueeze(0)).squeeze()\n",
        "\n",
        "                yhat = self.model.predict(X.unsqueeze(0))\n",
        "                yhat = yhat.squeeze()\n",
        "\n",
        "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
        "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
        "\n",
        "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
        "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
        "\n",
        "            if plot==True:\n",
        "                plt.plot(y_predicted, label='predicted')\n",
        "                plt.plot(y_true, label='actual')\n",
        "                plt.title(f\"k-steps = {k_steps}\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
        "            mae = mean_absolute_error(y_true, y_predicted)\n",
        "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
        "\n",
        "            rmse_list.append(rmse)\n",
        "            mae_list.append(mae)\n",
        "            mape_list.append(mape)\n",
        "\n",
        "            y_predicted_list.append(y_predicted)\n",
        "            y_true_list.append(y_true)\n",
        "\n",
        "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLj6VKl3_fDD"
      },
      "source": [
        "# Select city/province for prediction\n",
        "city = 'Bình Phước'\n",
        "\n",
        "# Pre-process data\n",
        "specific_data = get_city_data(fix_text(city))\n",
        "specific_data = impute_missing_value(specific_data)\n",
        "specific_data = convert_to_stationary(specific_data)\n",
        "specific_data.dropna(inplace=True)\n",
        "train, test = split_data(specific_data)\n",
        "\n",
        "# Fit data scaler to training data\n",
        "full_scaler = MinMaxScaler().fit(train)\n",
        "y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
        "\n",
        "# Scale train and test data\n",
        "train = full_scaler.transform(train)\n",
        "test = full_scaler.transform(test)\n",
        "\n",
        "# Get data to run model\n",
        "important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data)\n",
        "\n",
        "# Generate LSTM-ATT model and define province-specific params\n",
        "trainer = Trainer(model_type='lstm_attention',\n",
        "                          learning_rate=1e-3,\n",
        "                          important_features=important_features,\n",
        "                          train_loader=train_loader,\n",
        "                          test_tensor=test_tensor,\n",
        "                          n_layers=2,\n",
        "                          hidden_size=256)\n",
        "\n",
        "# # Generate LSTM model and define province-specific params \n",
        "# trainer = Trainer(model_type='lstm',\n",
        "#                           learning_rate=1e-3,\n",
        "#                           important_features=important_features,\n",
        "#                           train_loader=train_loader,\n",
        "#                           test_tensor=test_tensor,\n",
        "#                           n_layers=2,\n",
        "#                           hidden_size=512)\n",
        "\n",
        "# # Generate CNN model and define province-specific params\n",
        "# trainer = Trainer(type_model='cnn',\n",
        "#                   learning_rate=1e-3,\n",
        "#                   important_features=important_features,\n",
        "#                   train_loader=train_loader,\n",
        "#                   test_tensor=test_tensor,\n",
        "#                   num_filters=[100, 100, 100], \n",
        "#                   dropout=0.1)\n",
        "\n",
        "# Train model\n",
        "trainer.train(epochs=args.epochs)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluate on test set: \")\n",
        "y_true, y_pred, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot=True, scaled=True, city=city)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J27ZwfLYxJ19"
      },
      "source": [
        "# Print important prediction features for province\n",
        "for i in important_features:\n",
        "  print(specific_data.iloc[:,i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg7iFbkHAzSc"
      },
      "source": [
        "# Save observed rates, prediction values, and error metrics for 1-3 months\n",
        "results = pd.DataFrame({'Observed': y_true[0], \n",
        "                        '1-month': y_pred[0], \n",
        "                        '2-months': y_pred[1],\n",
        "                        '3-months': y_pred[2],\n",
        "                        'RMSE_1-month': rmse_list[0],\n",
        "                        'RMSE_2-month': rmse_list[1],\n",
        "                        'RMSE_3-month': rmse_list[2],\n",
        "                        'MAE_1-month': mae_list[0],\n",
        "                        'MAE_2-month': mae_list[1],\n",
        "                        'MAE_3-month': mae_list[2],\n",
        "                        'MAPE_1-month': mape_list[0],\n",
        "                        'MAPE_2-month': mape_list[1],\n",
        "                        'MAPE_3-month': mape_list[2]})\n",
        "print(results)\n",
        "# results.to_excel('df_binh_phuoc_LSTM-ATT.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}